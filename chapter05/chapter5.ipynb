{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e9225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://nlp100.github.io/data/ai.ja.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eaec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ai.ja.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9677c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ai.ja.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#邪魔な改行があるので、取り除く\n",
    "with open('ai.ja.txt') as fi, open('ai.ja.r.txt', 'w') as fw:\n",
    "    for l in fi:\n",
    "        if l != '\\n':\n",
    "            fw.write(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cabocha -f1 ai.ja.r.txt > ai.ja.txt.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ginza ai.ja.r.txt > ai.ja.g.txt # 1文ごとにEOSが付与されていないので、1分ごとに分割したい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b59390",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('work/ai.ja.g.txt') as f:\n",
    "    for l in f:\n",
    "        l =l.rstrip()\n",
    "        if '# text = 。' == l:\n",
    "            print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "ec98a368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ginza in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (5.1.2)\n",
      "Collecting ja_ginza\n",
      "  Using cached ja_ginza-5.1.2-py3-none-any.whl (59.1 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.2.0 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from ginza) (3.4.4)\n",
      "Requirement already satisfied: plac>=1.3.3 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from ginza) (1.3.5)\n",
      "Requirement already satisfied: SudachiPy<0.7.0,>=0.6.2 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from ginza) (0.6.6)\n",
      "Requirement already satisfied: SudachiDict-core>=20210802 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from ginza) (20230110)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (8.1.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (2.4.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (1.23.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (1.10.4)\n",
      "Requirement already satisfied: jinja2 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (67.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from spacy<3.5.0,>=3.2.0->ginza) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.2.0->ginza) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.2.0->ginza) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.2.0->ginza) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.2.0->ginza) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.2.0->ginza) (1.26.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.2.0->ginza) (2022.6.15)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.2.0->ginza) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.2.0->ginza) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.2.0->ginza) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nozueshinnosuke/.pyenv/versions/3.10.5/envs/100knock/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.2.0->ginza) (2.1.1)\n",
      "Installing collected packages: ja_ginza\n",
      "Successfully installed ja_ginza-5.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U ginza ja_ginza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a556c5ea",
   "metadata": {},
   "source": [
    "with open('work/ai.ja.g.txt') as f:\n",
    "    for l in f:\n",
    "        l =l.rstrip()\n",
    "        if '# text = ' in l and l[-1] != '。':\n",
    "            print(l)\n",
    "\n",
    "/# text = 人工知能\n",
    "/# text = ハンス・モラベックは、障害物の\n",
    "/# text = 2006年に、ジェフリー・ヒントンらの研究チームによりオートエンコーダによるニューラルネットワークの深層化手法が提案された（現在のディープラーニングの直接的な起源\n",
    "/# text = ）\n",
    "/# text = (BERT、ROBERT。)\n",
    "/# text = 科学と哲学\n",
    "\n",
    "特にやばいのが\n",
    "/# text = 2006年に、ジェフリー・ヒントンらの研究チームによりオートエンコーダによるニューラルネットワークの深層化手法が提案された（現在のディープラーニングの直接的な起源\n",
    "/# text = ）\n",
    "この次にこれがくるから３つくっつける形で対処\n",
    "/# text = 。\n",
    "\n",
    "/# text = ハンス・モラベックは、障害物の に関しては\n",
    "その次のやつをこれにくっつける形で対処\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a7be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit = ['2006年に、ジェフリー・ヒントンらの研究チームによりオートエンコーダによるニューラルネットワークの深層化手法が提案された（現在のディープラーニングの直接的な起源'\\\n",
    "    , '）', 'ハンス・モラベックは、障害物の', '2019年に入るとこれまで深層学習では困難とされてきた言語処理において大きな進展があり、Wikipediaなどを使用した読解テストで人間を上回るに至った。']\n",
    "store = ''\n",
    "is_hit = False\n",
    "    \n",
    "\n",
    "with open('work/ai.ja.g.txt') as fi, open('work/ai.ja.s.txt', 'w') as fw:\n",
    "    for l in fi:\n",
    "        if '# text = ' in l:\n",
    "            l = l.rstrip().replace('# text = ', '')\n",
    "            if l in hit:\n",
    "                store += l\n",
    "                is_hit = True\n",
    "            elif is_hit == True:\n",
    "                store += l\n",
    "                fw.write(store + '\\n')\n",
    "                is_hit = False\n",
    "                store = ''\n",
    "            else:\n",
    "                fw.write(l + '\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a00bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cabocha -f1 work/ai.ja.s.txt > work/ai.ja.txt.parsed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d99d6473",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，係り受け解析の結果（ai.ja.txt.parsed）を読み込み，各文をMorphオブジェクトのリストとして表現し，冒頭の説明文の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f46e75c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, l):\n",
    "        self.surface = l[0]\n",
    "        self.base = l[7]\n",
    "        self.pos = l[1]\n",
    "        self.pos1 = l[2]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'surface: {self.surface}\\tbase: {self.base}\\tpos: {self.pos}\\tpos1: {self.pos1}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "cd1a8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "split = re.compile('[\\t, ]')\n",
    "\n",
    "with open('work/ai.ja.txt.parsed') as f:\n",
    "    is_EOS = False\n",
    "    for l in f:\n",
    "        l = split.split(l.rstrip())\n",
    "        if is_EOS:\n",
    "            assert len(l) in [1, 5, 8, 10],'exception' # 自分の想定したlの長さ以外のものがないか確認\n",
    "            if l[0] == 'EOS':\n",
    "                break\n",
    "            elif len(l) >= 8:\n",
    "                sentence.append(Morph(l))\n",
    "        else:\n",
    "            if l[0] == 'EOS':\n",
    "                is_EOS = True\n",
    "                sentence = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3e5f8970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface: 人工\tbase: 人工\tpos: 名詞\tpos1: 一般\n",
      "surface: 知能\tbase: 知能\tpos: 名詞\tpos1: 一般\n",
      "surface: （\tbase: （\tpos: 記号\tpos1: 括弧開\n",
      "surface: じん\tbase: じん\tpos: 名詞\tpos1: 一般\n",
      "surface: こうち\tbase: こうち\tpos: 名詞\tpos1: 一般\n",
      "surface: のう\tbase: のう\tpos: 助詞\tpos1: 終助詞\n",
      "surface: 、\tbase: 、\tpos: 記号\tpos1: 読点\n",
      "surface: 、\tbase: 、\tpos: 記号\tpos1: 読点\n",
      "surface: AI\tbase: *\tpos: 名詞\tpos1: 一般\n",
      "surface: 〈\tbase: 〈\tpos: 記号\tpos1: 括弧開\n",
      "surface: エーアイ\tbase: *\tpos: 名詞\tpos1: 固有名詞\n",
      "surface: 〉\tbase: 〉\tpos: 記号\tpos1: 括弧閉\n",
      "surface: ）\tbase: ）\tpos: 記号\tpos1: 括弧閉\n",
      "surface: と\tbase: と\tpos: 助詞\tpos1: 格助詞\n",
      "surface: は\tbase: は\tpos: 助詞\tpos1: 係助詞\n",
      "surface: 、\tbase: 、\tpos: 記号\tpos1: 読点\n",
      "surface: 「\tbase: 「\tpos: 記号\tpos1: 括弧開\n",
      "surface: 『\tbase: 『\tpos: 記号\tpos1: 括弧開\n",
      "surface: 計算\tbase: 計算\tpos: 名詞\tpos1: サ変接続\n",
      "surface: （\tbase: （\tpos: 記号\tpos1: 括弧開\n",
      "surface: ）\tbase: ）\tpos: 記号\tpos1: 括弧閉\n",
      "surface: 』\tbase: 』\tpos: 記号\tpos1: 括弧閉\n",
      "surface: という\tbase: という\tpos: 助詞\tpos1: 格助詞\n",
      "surface: 概念\tbase: 概念\tpos: 名詞\tpos1: 一般\n",
      "surface: と\tbase: と\tpos: 助詞\tpos1: 並立助詞\n",
      "surface: 『\tbase: 『\tpos: 記号\tpos1: 括弧開\n",
      "surface: コンピュータ\tbase: コンピュータ\tpos: 名詞\tpos1: 一般\n",
      "surface: （\tbase: （\tpos: 記号\tpos1: 括弧開\n",
      "surface: ）\tbase: ）\tpos: 記号\tpos1: 括弧閉\n",
      "surface: 』\tbase: 』\tpos: 記号\tpos1: 括弧閉\n",
      "surface: という\tbase: という\tpos: 助詞\tpos1: 格助詞\n",
      "surface: 道具\tbase: 道具\tpos: 名詞\tpos1: 一般\n",
      "surface: を\tbase: を\tpos: 助詞\tpos1: 格助詞\n",
      "surface: 用い\tbase: 用いる\tpos: 動詞\tpos1: 自立\n",
      "surface: て\tbase: て\tpos: 助詞\tpos1: 接続助詞\n",
      "surface: 『\tbase: 『\tpos: 記号\tpos1: 括弧開\n",
      "surface: 知能\tbase: 知能\tpos: 名詞\tpos1: 一般\n",
      "surface: 』\tbase: 』\tpos: 記号\tpos1: 括弧閉\n",
      "surface: を\tbase: を\tpos: 助詞\tpos1: 格助詞\n",
      "surface: 研究\tbase: 研究\tpos: 名詞\tpos1: サ変接続\n",
      "surface: する\tbase: する\tpos: 動詞\tpos1: 自立\n",
      "surface: 計算\tbase: 計算\tpos: 名詞\tpos1: サ変接続\n",
      "surface: 機\tbase: 機\tpos: 名詞\tpos1: 接尾\n",
      "surface: 科学\tbase: 科学\tpos: 名詞\tpos1: 一般\n",
      "surface: （\tbase: （\tpos: 記号\tpos1: 括弧開\n",
      "surface: ）\tbase: ）\tpos: 記号\tpos1: 括弧閉\n",
      "surface: の\tbase: の\tpos: 助詞\tpos1: 連体化\n",
      "surface: 一\tbase: 一\tpos: 名詞\tpos1: 数\n",
      "surface: 分野\tbase: 分野\tpos: 名詞\tpos1: 一般\n",
      "surface: 」\tbase: 」\tpos: 記号\tpos1: 括弧閉\n",
      "surface: を\tbase: を\tpos: 助詞\tpos1: 格助詞\n",
      "surface: 指す\tbase: 指す\tpos: 動詞\tpos1: 自立\n",
      "surface: 語\tbase: 語\tpos: 名詞\tpos1: 一般\n",
      "surface: 。\tbase: 。\tpos: 記号\tpos1: 句点\n"
     ]
    }
   ],
   "source": [
    "for m in sentence:\n",
    "    print(m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04f731f9",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストの係り受け解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，冒頭の説明文の文節の文字列と係り先を表示せよ．本章の残りの問題では，ここで作ったプログラムを活用せよ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "fdc79155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, count):\n",
    "        self.idx: int = count\n",
    "        self.chunks = []\n",
    "        self.dst_srcs = {}\n",
    "        self.paths = {}\n",
    "\n",
    "    def show_dst_src(self):\n",
    "        deps = []\n",
    "        for c in self.chunks:\n",
    "            if c.dst == -1:\n",
    "                break\n",
    "            deps.append(f'{c.except_sig()}   {self.chunks[c.dst].except_sig()}')\n",
    "        return deps\n",
    "    \n",
    "    def show_nv_dep(self):\n",
    "        for c in self.chunks:\n",
    "            if c.dst == -1:\n",
    "                break\n",
    "            if c.has_pos('名詞') and self.chunks[c.dst].has_pos('動詞'):\n",
    "                print(f'{c.except_sig()}\\t{self.chunks[c.dst].except_sig()}')\n",
    "\n",
    "    def show_vp_dep(self):\n",
    "        for d in self.dst_srcs:\n",
    "            dst = self.chunks[d]\n",
    "            if dst.has_pos('動詞'):\n",
    "                particles = {self.chunks[s].extract_particle() for s in self.dst_srcs[d] if self.chunks[s].extract_particle()}\n",
    "                particles = ' '.join(sorted(particles))\n",
    "                print(f'{dst.initial_verb()}    {particles}')\n",
    "\n",
    "    def show_path(self, id):\n",
    "        if id in self.paths:\n",
    "            return self.paths[id]\n",
    "        \n",
    "        else:\n",
    "            if self.chunks[id].dst == -1:\n",
    "                self.paths[id] = self.chunks[id].except_sig()\n",
    "            else:\n",
    "                self.paths[id] = self.chunks[id].except_sig() + ' -> ' + self.show_path(self.chunks[id].dst)\n",
    "            return self.paths[id]\n",
    "        \n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return ''.join(c.except_sig() for c in self.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "0ee737e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, boc, sentence):\n",
    "        self.id: int = boc[1]\n",
    "        self.dst: int = boc[2]\n",
    "        self.morphs = []\n",
    "        self.srcs = []\n",
    "        self.sentence = sentence\n",
    "    \n",
    "    def except_sig(self):\n",
    "        return ''.join(m.surface for m in self.morphs if m.pos1 not in ['句点', '読点', '括弧開', '括弧閉'])\n",
    "    \n",
    "    def has_pos(self, pos):\n",
    "        for m in self.morphs:\n",
    "            if m.pos == pos:\n",
    "                return True\n",
    "            \n",
    "    def has_pos1(self, pos1):\n",
    "        for m in self.morphs:\n",
    "            if m.pos1 == pos1:\n",
    "                return True\n",
    "            \n",
    "    def initial_verb(self):\n",
    "        result = []\n",
    "        for m in self.morphs:\n",
    "            if m.pos == '動詞':\n",
    "                result.append(m.base)\n",
    "                return ''.join(result)\n",
    "            \n",
    "    def extract_particle(self):\n",
    "        particle = []\n",
    "        for m in self.morphs[-1::-1]:\n",
    "            if m.pos == '助詞':\n",
    "                particle.append(m.surface)\n",
    "            else:\n",
    "                return ''.join(particle[-1::-1])\n",
    "            \n",
    "    def noun_to_xy(self, alt, mode):\n",
    "        result = []\n",
    "        for m in self.morphs:\n",
    "            if m.pos in ['名詞','代名詞','記号']:\n",
    "                if m.pos == '記号':\n",
    "                    if m.pos1 not in ['句点', '読点', '括弧開', '括弧閉']:\n",
    "                        result.append(m.surface)\n",
    "                else:\n",
    "                    result.append(m.surface)\n",
    "        if len(result) == 1:\n",
    "            result = result[0]\n",
    "        else:\n",
    "            result = ''.join(result)\n",
    "        result = self.except_sig().replace(result, alt)\n",
    "        if mode == 'path':\n",
    "            target = self.except_sig()\n",
    "            return self.sentence.paths[self.id].replace(target, result)\n",
    "        elif mode == 'chunk':\n",
    "            return result\n",
    "        else:\n",
    "            assert mode in ['path', 'chunk'], 'mode is invalid'\n",
    "        \n",
    "    def __str__(self):\n",
    "        morph_join = ''.join(m.surface for m in self.morphs)\n",
    "        return f'id: {self.id}\\tmorphs: {morph_join}\\tdst: {self.dst}\\tsrcs: {self.srcs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "b72a3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = re.compile('[\\t, ]')\n",
    "document = []\n",
    "dst_srcs = {}\n",
    "count = 0\n",
    "\n",
    "\n",
    "with open('work/ai.ja.txt.parsed') as f:\n",
    "    for l in f:\n",
    "        l = split.split(l.rstrip())\n",
    "        if l[0] == 'EOS':\n",
    "            document.append(sentence)\n",
    "            sentence.dst_srcs = dst_srcs\n",
    "            dst_srcs = {}\n",
    "            \n",
    "        elif len(l) == 5: # len == 5 は boc 以外ないことを確認済み\n",
    "            l[1] = int(l[1])\n",
    "            l[2] = int(l[2][:-1]) # assertの結果'D'以外はないとわかったので\n",
    "            if l[1] == 0:\n",
    "                sentence = Sentence(count)\n",
    "                count += 1\n",
    "            sentence.chunks.append(Chunk(l, sentence))\n",
    "\n",
    "            if l[2] in dst_srcs: # dstのchunkは後に来るので保存しておきたい\n",
    "                dst_srcs[l[2]].append(l[1])\n",
    "            elif l[2] != -1:\n",
    "                dst_srcs[l[2]] = [l[1]]\n",
    "                \n",
    "            if l[1] in dst_srcs: # chunkができる頃にはsrcは揃ってるからchunkを作った時に入れ込む\n",
    "                sentence.chunks[-1].srcs = dst_srcs[l[1]]\n",
    "                \n",
    "        else:\n",
    "            sentence.chunks[-1].morphs.append(Morph(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "id": "307afdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 0\tmorphs: 人工知能\tdst: 17\tsrcs: []\n",
      "id: 1\tmorphs: （じんこうちのう、、\tdst: 17\tsrcs: []\n",
      "id: 2\tmorphs: AI\tdst: 3\tsrcs: []\n",
      "id: 3\tmorphs: 〈エーアイ〉）とは、\tdst: 17\tsrcs: [2]\n",
      "id: 4\tmorphs: 「『計算\tdst: 5\tsrcs: []\n",
      "id: 5\tmorphs: （）』という\tdst: 9\tsrcs: [4]\n",
      "id: 6\tmorphs: 概念と\tdst: 9\tsrcs: []\n",
      "id: 7\tmorphs: 『コンピュータ\tdst: 8\tsrcs: []\n",
      "id: 8\tmorphs: （）』という\tdst: 9\tsrcs: [7]\n",
      "id: 9\tmorphs: 道具を\tdst: 10\tsrcs: [5, 6, 8]\n",
      "id: 10\tmorphs: 用いて\tdst: 12\tsrcs: [9]\n",
      "id: 11\tmorphs: 『知能』を\tdst: 12\tsrcs: []\n",
      "id: 12\tmorphs: 研究する\tdst: 13\tsrcs: [10, 11]\n",
      "id: 13\tmorphs: 計算機科学\tdst: 14\tsrcs: [12]\n",
      "id: 14\tmorphs: （）の\tdst: 15\tsrcs: [13]\n",
      "id: 15\tmorphs: 一分野」を\tdst: 16\tsrcs: [14]\n",
      "id: 16\tmorphs: 指す\tdst: 17\tsrcs: [15]\n",
      "id: 17\tmorphs: 語。\tdst: -1\tsrcs: [0, 1, 3, 16]\n"
     ]
    }
   ],
   "source": [
    "for c in document[1].chunks:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "id": "de874c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import islice\n",
    "\n",
    "def generate_sentence(): # 起動前にimport re\n",
    "    sentence = Sentence()\n",
    "    split = re.compile('[\\t, ]')\n",
    "    alpha = re.compile('[a-zA-Z]')\n",
    "    is_EOS = False\n",
    "    dst_srcs = {}\n",
    "\n",
    "    with open('work/ai.ja.txt.parsed') as f:\n",
    "        for l in f:\n",
    "            if is_EOS:\n",
    "                yield sentence\n",
    "            l = split.split(l.rstrip())\n",
    "            if l[0] == 'EOS':\n",
    "                is_EOS = True\n",
    "                sentence.dst_srcs = dst_srcs\n",
    "                \n",
    "            elif len(l) == 5: # len == 5 は boc 以外ないことを確認済み, l[0] == *にすると*で始まる形態素があったら困る\n",
    "                l[1] = int(l[1])\n",
    "                l[2] = int(alpha.sub('', l[2])) \n",
    "                sentence.chunks.append(Chunk(l, sentence))\n",
    "\n",
    "                if l[2] in dst_srcs: # dstのchunkは後に来るので保存しておきたい\n",
    "                    dst_srcs[l[2]].append(l[1])\n",
    "                elif l[2] != -1:\n",
    "                    dst_srcs[l[2]] = [l[1]]\n",
    "                    \n",
    "                if l[1] in dst_srcs: # chunkができる頃にはsrcは揃ってるからchunkを作った時に入れ込む\n",
    "                    sentence.chunks[-1].srcs = dst_srcs[l[1]]\n",
    "                    \n",
    "            else:\n",
    "                sentence.chunks[-1].morphs.append(Morph(l))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "923c22ff",
   "metadata": {},
   "source": [
    "## 42. 係り元と係り先の文節の表示\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "id": "fb3cfbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['人工知能   語',\n",
       " 'じんこうちのう   語',\n",
       " 'AI   エーアイとは',\n",
       " 'エーアイとは   語',\n",
       " '計算   という',\n",
       " 'という   道具を',\n",
       " '概念と   道具を',\n",
       " 'コンピュータ   という',\n",
       " 'という   道具を',\n",
       " '道具を   用いて',\n",
       " '用いて   研究する',\n",
       " '知能を   研究する',\n",
       " '研究する   計算機科学',\n",
       " '計算機科学   の',\n",
       " 'の   一分野を',\n",
       " '一分野を   指す',\n",
       " '指す   語']"
      ]
     },
     "execution_count": 866,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[1].show_dst_src()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1979a6f2",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "id": "8391fb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "知的行動を\t代わって\n",
      "人間に\t代わって\n",
      "コンピューターに\t行わせる\n",
      "研究分野とも\tされる\n"
     ]
    }
   ],
   "source": [
    "document[2].show_nv_dep()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2032c064",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，Graphviz等を用いるとよい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "fb731a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.png'"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "\n",
    "g = graphviz.Digraph(format='png')\n",
    "for c in document[1].chunks:\n",
    "    if c.dst == -1:\n",
    "        break\n",
    "    src = str(c.id)\n",
    "    dst = str(c.dst)\n",
    "    g.node(label=c.except_sig(), name=src)\n",
    "    g.node(label=document[1].chunks[c.dst].except_sig(), name=dst)\n",
    "    g.edge(src, dst)\n",
    "\n",
    "g.view()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20780d1b",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "・動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "・述語に係る助詞を格とする\n",
    "・述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "id": "ba6de3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: [0, 1], 3: [2], 4: [3], 6: [4, 5], 8: [6, 7], 9: [8], 16: [9, 15], 11: [10], 13: [11, 12], 15: [13, 14], 17: [16]}\n"
     ]
    }
   ],
   "source": [
    "print(document[2].dst_srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "id": "fa8103b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('work/vp_dep.txt', 'w') as fo:\n",
    "    for sen in document:\n",
    "        for d in sen.dst_srcs:\n",
    "            dst = sen.chunks[d]\n",
    "            if dst.has_pos('動詞'):\n",
    "                cases = {sen.chunks[src].extract_particle() for src in sen.dst_srcs[d] if sen.chunks[src].extract_particle()}\n",
    "                if cases:\n",
    "                    cases = ' '.join(sorted(cases))\n",
    "                    fo.write(f'{dst.initial_verb()}\\t{cases}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "id": "92692982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     59 する\tを\n",
      "     27 する\tが\n",
      "     26 する\tと\n",
      "     19 する\tに\n",
      "     12 する\tは を\n",
      "     11 する\tに を\n",
      "     10 する\tで を\n",
      "     10 よる\tに\n",
      "      9 行う\tを\n",
      "      7 する\tが に\n"
     ]
    }
   ],
   "source": [
    "!cat work/vp_dep.txt | sort | uniq -c | sort -nr | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "id": "dab75a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9 行う\tを\n",
      "      3 行う\tに を\n",
      "      1 行う\tまで を\n",
      "      1 行う\tには は\n",
      "      1 行う\tから\n",
      "      1 行う\tに まで を\n",
      "      1 行う\tが では\n",
      "      1 行う\tは を をめぐって\n",
      "      1 行う\tが で には\n",
      "      1 行う\tで に を\n",
      "      1 行う\tは を\n",
      "      1 行う\tで を\n",
      "      1 行う\tて に\n",
      "      1 行う\tが に\n",
      "      1 行う\tに\n"
     ]
    }
   ],
   "source": [
    "!grep -E \"^行う\" work/vp_dep.txt | sort | uniq -c | sort -nr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "id": "edec7f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4 なる\tに は\n",
      "      4 なる\tが と\n",
      "      2 なる\tは\n",
      "      2 なる\tに\n",
      "      1 なる\tとして に は\n",
      "      1 なる\tから が て では と\n",
      "      1 なる\tには も\n",
      "      1 なる\tでは と\n",
      "      1 なる\tが とは にとって\n",
      "      1 なる\tで に には\n",
      "      1 なる\tが に には\n",
      "      1 なる\tが で と に は\n",
      "      1 なる\tと に\n",
      "      1 なる\tで と\n",
      "      1 なる\tが に\n",
      "      1 なる\tも\n",
      "      1 なる\tと\n"
     ]
    }
   ],
   "source": [
    "!grep -E \"^なる\" work/vp_dep.txt | sort | uniq -c | sort -nr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "ae62bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 与える\tが などに\n",
      "      1 与える\tに は を\n",
      "      1 与える\tが に\n"
     ]
    }
   ],
   "source": [
    "!grep -E \"^与える\" work/vp_dep.txt | sort | uniq -c | sort -nr "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1afd4a4a",
   "metadata": {},
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "・項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "・述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "id": "c8412ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作り出す\tで は を\t会議で ジョン・マッカーシーは 用語を\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# もうちょいかっこよく書きたい\n",
    "with open('work/predicate_argument.txt', 'w') as fo:\n",
    "    for sen in document:\n",
    "        for d in sen.dst_srcs:\n",
    "            dst = sen.chunks[d]\n",
    "            if dst.has_pos('動詞'):\n",
    "                arguments_info = [sen.chunks[src] for src in sen.dst_srcs[d] if sen.chunks[src].extract_particle()]\n",
    "                if arguments_info:\n",
    "                    arguments_info = sorted(arguments_info, key = lambda x: x.extract_particle())\n",
    "                    cases, arguments = zip(*[(c.extract_particle(), c.except_sig()) for c in arguments_info])\n",
    "                    cases = ' '.join(cases)\n",
    "                    arguments = ' '.join(arguments)    \n",
    "                    fo.write(f'{dst.initial_verb()}\\t{cases}\\t{arguments}\\n')\n",
    "                    \n",
    "                    if sen == document[33]:\n",
    "                        print(''.join(f'{dst.initial_verb()}\\t{cases}\\t{arguments}\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "9c10fbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用いる\tを\t道具を\n",
      "する\tて を\t用いて 知能を\n",
      "指す\tを\t一分野を\n",
      "代わる\tに を\t人間に 知的行動を\n",
      "行う\tて に\t代わって コンピューターに\n",
      "する\tとも\t研究分野とも\n",
      "述べる\tに は\t次のように 佐藤理史は\n",
      "する\tで を\tコンピュータ上で 知的能力を\n",
      "ある\tが は\t画像認識等が 応用例は\n",
      "する\tを\t推論・判断を\n"
     ]
    }
   ],
   "source": [
    "!head -10 work/predicate_argument.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "756674cd",
   "metadata": {},
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "・「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "・述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "・述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "・述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "fcf82ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6: [0, 3, 5], 2: [1], 3: [2], 5: [4]}\n"
     ]
    }
   ],
   "source": [
    "print(document[33].dst_srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "id": "33dd5681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "で\n",
      "を\n"
     ]
    }
   ],
   "source": [
    "print(document[33].chunks[3].extract_particle())\n",
    "print(document[33].chunks[5].extract_particle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "59743234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "会議で\n",
      "用語を\n"
     ]
    }
   ],
   "source": [
    "print(document[33].chunks[3].except_sig())\n",
    "print(document[33].chunks[5].except_sig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "8c9a238b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作り出す\n"
     ]
    }
   ],
   "source": [
    "print(document[33].chunks[6].initial_verb())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "172a8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題の例(間違った解析になってるけど)や、「〜を、または、〇〇をする」みたいな文があったら、どちらのパターンもサ変接続名詞として扱いたい\n",
    "\n",
    "with open('work/sahen.txt', 'w') as fo:\n",
    "    for sen in document:\n",
    "        for d in sen.dst_srcs:\n",
    "            dst = sen.chunks[d]\n",
    "\n",
    "            if dst.has_pos('動詞'):\n",
    "                arguments_chunks = [sen.chunks[src] for src in sen.dst_srcs[d] if sen.chunks[src].extract_particle()]\n",
    "\n",
    "                if len(arguments_chunks) > 1:\n",
    "                    arguments_chunks = sorted(arguments_chunks, key = lambda x: x.extract_particle())\n",
    "                    for chunk in arguments_chunks:\n",
    "                        if chunk.extract_particle() == 'を' and chunk.has_pos1('サ変接続'):    # 'を'が来た時、その説を動詞にくっつけて、他の節をtab区切りで出力\n",
    "                            sahen = chunk.except_sig() + dst.initial_verb()\n",
    "                            cases, arguments = zip(*[(c.extract_particle(), c.except_sig()) for c in arguments_chunks if c != chunk])\n",
    "                            cases = ' '.join(cases)\n",
    "                            arguments = ' '.join(arguments)\n",
    "                            fo.write(f'{sahen}\\t{cases}\\t{arguments}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "d5a7fbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "知的行動を代わる\tに\t人間に\n",
      "記述をする\tと\t主体と\n",
      "注目を集める\tが\tサポートベクターマシンが\n",
      "経験を行う\tに を\t元に 学習を\n",
      "統計的学習をする\tに を通して\t元に 生成規則を通して\n",
      "進化を見せる\tにおいて\t生成技術において\n",
      "機械式計算機をする\tは\tブレーズ・パスカルは\n",
      "開発を行う\tは\tエイダ・ラブレスは\n",
      "プログラミング言語をする\tは\tアラン・カルメラウアーは\n",
      "バックギャモン専用コンピュータ・TDギャモンをする\tに は\t1992年に IBMは\n",
      "投資全額を上回る\tが\tコストが\n",
      "意味付けをする\tから\tここから\n",
      "知的処理を行う\tに\tWebに\n",
      "意味をする\tに\tデータに\n",
      "知的処理を行う\tに\tコンピュータに\n",
      "研究を進める\tて\t費やして\n",
      "命令をする\tで\t機構で\n",
      "運転をする\tに\t元に\n",
      "特許をする\tが までに\t日本が 2018年までに\n",
      "運転をする\tに\t柔軟に\n"
     ]
    }
   ],
   "source": [
    "!head -20 work/sahen.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "802de520",
   "metadata": {},
   "source": [
    "## 48. 名詞から根へのパスの抽出\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "・各文節は（表層形の）形態素列で表現する\n",
    "・パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "id": "34fec966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用語を -> 作り出した\n",
      "人工知能という -> 用語を -> 作り出した\n",
      "会議で -> 作り出した\n",
      "最初の -> 会議で -> 作り出した\n",
      "AIに関する -> 最初の -> 会議で -> 作り出した\n",
      "ジョン・マッカーシーは -> 作り出した\n"
     ]
    }
   ],
   "source": [
    "# 構文木の根から順番に戻りつつ、パスを作りつつ、保存する\n",
    "# 名詞があったら、作ったパスを出力用に保存する\n",
    "sentence = document[33]\n",
    "for c in sentence.chunks[::-1]:\n",
    "    if c.has_pos('名詞'):\n",
    "        print(sentence.show_path(c.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "id": "a9e524e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen in document:\n",
    "    for c in sen.chunks[::-1]:\n",
    "        if c.has_pos('名詞'):\n",
    "            sen.show_path(c.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "id": "2a758a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "応用例は -> ある\n",
      "自然言語処理 -> 機械翻訳・かな漢字変換・構文解析等 -> 専門家の -> 推論・判断を -> 模倣する -> エキスパートシステム -> 画像認識等が -> ある\n",
      "機械翻訳・かな漢字変換・構文解析等 -> 専門家の -> 推論・判断を -> 模倣する -> エキスパートシステム -> 画像認識等が -> ある\n",
      "専門家の -> 推論・判断を -> 模倣する -> エキスパートシステム -> 画像認識等が -> ある\n",
      "推論・判断を -> 模倣する -> エキスパートシステム -> 画像認識等が -> ある\n",
      "模倣する -> エキスパートシステム -> 画像認識等が -> ある\n",
      "エキスパートシステム -> 画像認識等が -> ある\n",
      "画像データを -> 解析して -> 検出・抽出したりする -> 画像認識等が -> ある\n",
      "解析して -> 検出・抽出したりする -> 画像認識等が -> ある\n",
      "特定の -> パターンを -> 検出・抽出したりする -> 画像認識等が -> ある\n",
      "パターンを -> 検出・抽出したりする -> 画像認識等が -> ある\n",
      "検出・抽出したりする -> 画像認識等が -> ある\n",
      "画像認識等が -> ある\n"
     ]
    }
   ],
   "source": [
    "sentence = document[5]\n",
    "for c in sentence.chunks:\n",
    "    if c.has_pos('名詞'):\n",
    "        print(sentence.show_path(c.id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62b06fbd",
   "metadata": {},
   "source": [
    "## 49. 名詞間の係り受けパスの抽出\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がiとj（i < j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "・問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
    "・文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "・文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n",
    "・上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を” | “で連結して表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "id": "30c31a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_path(x_chunk, y_chunk): \n",
    "    x_path = x_chunk.noun_to_xy('X', 'path').split(' -> ')\n",
    "    result_path = x_path[0]\n",
    "    for p in x_path[1:]:\n",
    "        if p == y_chunk.except_sig():\n",
    "            p = y_chunk.noun_to_xy('Y', 'chunk')\n",
    "            result_path += ' -> ' + p\n",
    "            if result_path:\n",
    "                return result_path\n",
    "        result_path += ' -> ' + p\n",
    "\n",
    "def other_paths(x_chunk, y_chunk):\n",
    "    x_path = x_chunk.noun_to_xy('X', 'path').split(' -> ')\n",
    "    y_path = y_chunk.noun_to_xy('Y', 'path').split(' -> ')\n",
    "    union_path = []\n",
    "    for x, y in zip(x_path[::-1], y_path[::-1]):\n",
    "        if x != y:\n",
    "            union_path = ' -> '.join(union_path[::-1])\n",
    "            x_path = ' -> '.join(x_path).replace(' -> ' + union_path, '')\n",
    "            y_path = ' -> '.join(y_path).replace(' -> ' + union_path, '')\n",
    "            result = x_path + ' | ' +  y_path + ' | ' + union_path\n",
    "            if result:\n",
    "                return  result\n",
    "        union_path.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "id": "220aee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "result = {}\n",
    "\n",
    "with open('work/noun_xy_path.txt', 'w') as fo:\n",
    "    for sentence in document:\n",
    "        result[sentence.idx] = []\n",
    "        for i, _ in enumerate(sentence.chunks):\n",
    "            x_chunk = sentence.chunks[i]\n",
    "            if x_chunk.has_pos('名詞'):\n",
    "                for c in sentence.chunks[i+1:]:\n",
    "                    if c.has_pos('名詞'):\n",
    "                        if c.except_sig() in sentence.show_path(i):\n",
    "                            result[sentence.idx].append(same_path(x_chunk, c))\n",
    "                        else:\n",
    "                            result[sentence.idx].append(other_paths(x_chunk, c))\n",
    "\n",
    "    json.dump(result, fo, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "id": "50dd74ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== 33 ======\n",
      "Xは | Yに関する -> 最初の -> 会議で | 作り出した\n",
      "Xは | Yの -> 会議で | 作り出した\n",
      "Xは | Yで | 作り出した\n",
      "Xは | Yという -> 用語を | 作り出した\n",
      "Xは | Yを | 作り出した\n",
      "Xに関する -> Yの\n",
      "Xに関する -> 最初の -> Yで\n",
      "Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
      "Xに関する -> 最初の -> 会議で | Yを | 作り出した\n",
      "Xの -> Yで\n",
      "Xの -> 会議で | Yという -> 用語を | 作り出した\n",
      "Xの -> 会議で | Yを | 作り出した\n",
      "Xで | Yという -> 用語を | 作り出した\n",
      "Xで | Yを | 作り出した\n",
      "Xという -> Yを\n",
      "====== 10 ======\n",
      "Xな -> Yの\n",
      "Xな -> 知能の -> Yへの\n",
      "Xな -> 知能の -> 実現への -> Yとしては\n",
      "Xな -> 知能の -> 実現への -> アプローチとしては | Yや -> ニューラルネットワークなどのような -> アプローチも | 知られているが -> ある\n",
      "Xな -> 知能の -> 実現への -> アプローチとしては | ニューラルネットワークなどのような -> アプローチも | 知られているが -> ある\n",
      "Xな -> 知能の -> 実現への -> アプローチとしては | Yも | 知られているが -> ある\n",
      "Xな -> 知能の -> 実現への -> アプローチとしては -> 知られているが | Yの -> 人工知能である -> (GoodOldFashionedAI)との -> 差は | ある\n",
      "Xな -> 知能の -> 実現への -> アプローチとしては -> 知られているが | Yである -> (GoodOldFashionedAI)との -> 差は | ある\n",
      "Xな -> 知能の -> 実現への -> アプローチとしては -> 知られているが | Yとの -> 差は | ある\n",
      "Xな -> 知能の -> 実現への -> アプローチとしては -> 知られているが | Yは | ある\n",
      "Xな -> 知能の -> 実現への -> アプローチとしては -> 知られているが | Yの -> 記号的明示性に | ある\n",
      "Xな -> 知能の -> 実現への -> アプローチとしては -> 知られているが | Yに | ある\n",
      "Xの -> Yへの\n",
      "Xの -> 実現への -> Yとしては\n",
      "Xの -> 実現への -> アプローチとしては | Yや -> ニューラルネットワークなどのような -> アプローチも | 知られているが -> ある\n",
      "Xの -> 実現への -> アプローチとしては | ニューラルネットワークなどのような -> アプローチも | 知られているが -> ある\n",
      "Xの -> 実現への -> アプローチとしては | Yも | 知られているが -> ある\n",
      "Xの -> 実現への -> アプローチとしては -> 知られているが | Yの -> 人工知能である -> (GoodOldFashionedAI)との -> 差は | ある\n",
      "Xの -> 実現への -> アプローチとしては -> 知られているが | Yである -> (GoodOldFashionedAI)との -> 差は | ある\n",
      "Xの -> 実現への -> アプローチとしては -> 知られているが | Yとの -> 差は | ある\n",
      "Xの -> 実現への -> アプローチとしては -> 知られているが | Yは | ある\n",
      "Xの -> 実現への -> アプローチとしては -> 知られているが | Yの -> 記号的明示性に | ある\n",
      "Xの -> 実現への -> アプローチとしては -> 知られているが | Yに | ある\n",
      "Xへの -> Yとしては\n",
      "Xへの -> アプローチとしては | Yや -> ニューラルネットワークなどのような -> アプローチも | 知られているが -> ある\n",
      "Xへの -> アプローチとしては | ニューラルネットワークなどのような -> アプローチも | 知られているが -> ある\n",
      "Xへの -> アプローチとしては | Yも | 知られているが -> ある\n",
      "Xへの -> アプローチとしては -> 知られているが | Yの -> 人工知能である -> (GoodOldFashionedAI)との -> 差は | ある\n",
      "Xへの -> アプローチとしては -> 知られているが | Yである -> (GoodOldFashionedAI)との -> 差は | ある\n",
      "Xへの -> アプローチとしては -> 知られているが | Yとの -> 差は | ある\n",
      "Xへの -> アプローチとしては -> 知られているが | Yは | ある\n",
      "Xへの -> アプローチとしては -> 知られているが | Yの -> 記号的明示性に | ある\n",
      "Xへの -> アプローチとしては -> 知られているが | Yに | ある\n",
      "Xとしては | Yや -> ニューラルネットワークなどのような -> アプローチも | 知られているが -> ある\n",
      "Xとしては | ニューラルネットワークなどのような -> アプローチも | 知られているが -> ある\n",
      "Xとしては | Yも | 知られているが -> ある\n",
      "Xとしては -> 知られているが | Yの -> 人工知能である -> (GoodOldFashionedAI)との -> 差は | ある\n",
      "Xとしては -> 知られているが | Yである -> (GoodOldFashionedAI)との -> 差は | ある\n",
      "Xとしては -> 知られているが | Yとの -> 差は | ある\n",
      "Xとしては -> 知られているが | Yは | ある\n",
      "Xとしては -> 知られているが | Yの -> 記号的明示性に | ある\n",
      "Xとしては -> 知られているが | Yに | ある\n",
      "Xや -> ニューラルネットワークなどのような\n",
      "Xや -> ニューラルネットワークなどのような -> Yも\n",
      "Xや -> ニューラルネットワークなどのような -> アプローチも -> 知られているが | Yの -> 人工知能である -> (GoodOldFashionedAI)との -> 差は | ある\n",
      "Xや -> ニューラルネットワークなどのような -> アプローチも -> 知られているが | Yである -> (GoodOldFashionedAI)との -> 差は | ある\n",
      "Xや -> ニューラルネットワークなどのような -> アプローチも -> 知られているが | Yとの -> 差は | ある\n",
      "Xや -> ニューラルネットワークなどのような -> アプローチも -> 知られているが | Yは | ある\n",
      "Xや -> ニューラルネットワークなどのような -> アプローチも -> 知られているが | Yの -> 記号的明示性に | ある\n",
      "Xや -> ニューラルネットワークなどのような -> アプローチも -> 知られているが | Yに | ある\n",
      "ニューラルネットワークなどのような -> Yも\n",
      "ニューラルネットワークなどのような -> アプローチも -> 知られているが | Yの -> 人工知能である -> (GoodOldFashionedAI)との -> 差は | ある\n",
      "ニューラルネットワークなどのような -> アプローチも -> 知られているが | Yである -> (GoodOldFashionedAI)との -> 差は | ある\n",
      "ニューラルネットワークなどのような -> アプローチも -> 知られているが | Yとの -> 差は | ある\n",
      "ニューラルネットワークなどのような -> アプローチも -> 知られているが | Yは | ある\n",
      "ニューラルネットワークなどのような -> アプローチも -> 知られているが | Yの -> 記号的明示性に | ある\n",
      "ニューラルネットワークなどのような -> アプローチも -> 知られているが | Yに | ある\n",
      "Xも -> 知られているが | Yの -> 人工知能である -> (GoodOldFashionedAI)との -> 差は | ある\n",
      "Xも -> 知られているが | Yである -> (GoodOldFashionedAI)との -> 差は | ある\n",
      "Xも -> 知られているが | Yとの -> 差は | ある\n",
      "Xも -> 知られているが | Yは | ある\n",
      "Xも -> 知られているが | Yの -> 記号的明示性に | ある\n",
      "Xも -> 知られているが | Yに | ある\n",
      "Xの -> Yである\n",
      "Xの -> 人工知能である -> Yとの\n",
      "Xの -> 人工知能である -> (GoodOldFashionedAI)との -> Yは\n",
      "Xの -> 人工知能である -> (GoodOldFashionedAI)との -> 差は | Yの -> 記号的明示性に | ある\n",
      "Xの -> 人工知能である -> (GoodOldFashionedAI)との -> 差は | Yに | ある\n",
      "Xである -> Yとの\n",
      "Xである -> (GoodOldFashionedAI)との -> Yは\n",
      "Xである -> (GoodOldFashionedAI)との -> 差は | Yの -> 記号的明示性に | ある\n",
      "Xである -> (GoodOldFashionedAI)との -> 差は | Yに | ある\n",
      "Xとの -> Yは\n",
      "Xとの -> 差は | Yの -> 記号的明示性に | ある\n",
      "Xとの -> 差は | Yに | ある\n",
      "Xは | Yの -> 記号的明示性に | ある\n",
      "Xは | Yに | ある\n",
      "Xの -> Yに\n"
     ]
    }
   ],
   "source": [
    "with open('work/noun_xy_path.txt') as fi:\n",
    "    result = json.load(fi)\n",
    "\n",
    "print('====== 33 ======') \n",
    "for v in result['33']:\n",
    "    print(v)\n",
    "\n",
    "print('====== 10 ======')\n",
    "for v in result['10']:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40612bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit ('100knock': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "395a96220ad211f52f45e991b75182650046c054c0e7a897ef373f2f51c85f5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
