{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (1.26.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./.venv/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in ./.venv/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.2.140)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting timm\n",
      "  Obtaining dependency information for timm from https://files.pythonhosted.org/packages/f0/1e/05287cb8984229d101874433df472b1fa3dcd6f746ccb6e8a26c7deeb1c7/timm-0.9.10-py3-none-any.whl.metadata\n",
      "  Downloading timm-0.9.10-py3-none-any.whl.metadata (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7 in ./.venv/lib/python3.10/site-packages (from timm) (2.1.0)\n",
      "Collecting torchvision (from timm)\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/84/eb/4f6483ae9094e164dc5b9b792e377f7d37823b0bedc3eef3193d416d2bb6/torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
      "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting pyyaml (from timm)\n",
      "  Obtaining dependency information for pyyaml from https://files.pythonhosted.org/packages/29/61/bf33c6c85c55bc45a29eee3195848ff2d518d84735eb0e2d8cb42e0d285e/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub (from timm)\n",
      "  Obtaining dependency information for huggingface-hub from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from timm)\n",
      "  Obtaining dependency information for safetensors from https://files.pythonhosted.org/packages/20/4e/878b080dbda92666233ec6f316a53969edcb58eab1aa399a64d0521cf953/safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (4.8.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in ./.venv/lib/python3.10/site-packages (from torch>=1.7->timm) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7->timm) (12.2.140)\n",
      "Collecting requests (from huggingface-hub->timm)\n",
      "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub->timm)\n",
      "  Obtaining dependency information for tqdm>=4.42.1 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from huggingface-hub->timm) (23.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from torchvision->timm) (1.26.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision->timm)\n",
      "  Obtaining dependency information for pillow!=8.3.*,>=5.3.0 from https://files.pythonhosted.org/packages/e5/b9/5c6ad3241f1ccca4b781dfeddbab2dac4480f95aedc351a0e60c9f4c8aa9/Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub->timm)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/da/f1/3702ba2a7470666a62fd81c58a4c40be00670e5006a67f4d626e57f013ae/charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub->timm)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub->timm)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/d2/b2/b157855192a68541a91ba7b2bbcb91f1b4faa51f8bae38d8005c034be524/urllib3-2.0.7-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub->timm)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
      "Downloading timm-0.9.10-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Using cached safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
      "Using cached Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, tqdm, safetensors, pyyaml, pillow, idna, charset-normalizer, certifi, requests, huggingface-hub, torchvision, timm\n",
      "Successfully installed certifi-2023.7.22 charset-normalizer-3.3.2 huggingface-hub-0.18.0 idna-3.4 pillow-10.1.0 pyyaml-6.0.1 requests-2.31.0 safetensors-0.4.0 timm-0.9.10 torchvision-0.16.0 tqdm-4.66.1 urllib3-2.0.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting wandb\n",
      "  Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/1c/5e/0362fa88679852c7fd3ac85ee5bd949426c4a51a61379010d4089be6d7ac/wandb-0.15.12-py3-none-any.whl.metadata\n",
      "  Downloading wandb-0.15.12-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting Click!=8.0.0,>=7.1 (from wandb)\n",
      "  Obtaining dependency information for Click!=8.0.0,>=7.1 from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Obtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/8d/c4/82b858fb6483dfb5e338123c154d19c043305b01726a67d89532b8f8f01b/GitPython-3.1.40-py3-none-any.whl.metadata\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./.venv/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./.venv/lib/python3.10/site-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Obtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/56/f7/d1d459caa0468217ab2638112c5ab31cde516aa3986f2ca292661644e6b8/sentry_sdk-1.34.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading sentry_sdk-1.34.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in ./.venv/lib/python3.10/site-packages (from wandb) (6.0.1)\n",
      "Collecting pathtools (from wandb)\n",
      "  Using cached pathtools-0.1.2-py3-none-any.whl\n",
      "Collecting setproctitle (from wandb)\n",
      "  Obtaining dependency information for setproctitle from https://files.pythonhosted.org/packages/79/e7/54b36be02aee8ad573be68f6f46fd62838735c2f007b22df50eb5e13a20d/setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Collecting setuptools (from wandb)\n",
      "  Obtaining dependency information for setuptools from https://files.pythonhosted.org/packages/bb/26/7945080113158354380a12ce26873dd6c1ebd88d47f5bc24e2c5bb38c16a/setuptools-68.2.2-py3-none-any.whl.metadata\n",
      "  Using cached setuptools-68.2.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,<5,>=3.19.0 from https://files.pythonhosted.org/packages/ae/32/45b1cf0c5d4a3ba881f5164c26af877c0dabfe6de0019d426aa0e5cf6806/protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: six>=1.4.0 in ./.venv/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Using cached wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.4/294.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-1.34.0-py2.py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.9/243.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Using cached setuptools-68.2.2-py3-none-any.whl (807 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pathtools, appdirs, smmap, setuptools, setproctitle, sentry-sdk, protobuf, docker-pycreds, Click, gitdb, GitPython, wandb\n",
      "Successfully installed Click-8.1.7 GitPython-3.1.40 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.11 pathtools-0.1.2 protobuf-4.25.0 sentry-sdk-1.34.0 setproctitle-1.3.3 setuptools-68.2.2 smmap-5.0.1 wandb-0.15.12\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install torch\n",
    "%pip install timm\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70. 単語ベクトルの和による特徴量\n",
    "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．細かな仕様は問題を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/work01/s-nozue/100knock/chapter08/chapter08.ipynb セル 2\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bamaretto02/work01/s-nozue/100knock/chapter08/chapter08.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bamaretto02/work01/s-nozue/100knock/chapter08/chapter08.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bamaretto02/work01/s-nozue/100knock/chapter08/chapter08.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m KeyedVectors\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bamaretto02/work01/s-nozue/100knock/chapter08/chapter08.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bamaretto02/work01/s-nozue/100knock/chapter08/chapter08.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mtorchの勉強がしたい\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bamaretto02/work01/s-nozue/100knock/chapter08/chapter08.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mX : 事例の特徴ベクトルを並べた行列\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bamaretto02/work01/s-nozue/100knock/chapter08/chapter08.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m        ・m: 健康\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bamaretto02/work01/s-nozue/100knock/chapter08/chapter08.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bamaretto02/work01/s-nozue/100knock/chapter08/chapter08.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# データの読み込み\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "'''\n",
    "torchの勉強がしたい\n",
    "X : 事例の特徴ベクトルを並べた行列\n",
    "    ・事例の特徴ベクトルとは\n",
    "        ・事例（タイトル）の単語ベクトル列を平均したもの\n",
    "        ・次元数は300\n",
    "y : 正解ラベルを並べたベクトル\n",
    "    ・4つのカテゴリ\n",
    "        ・0: ビジネス\n",
    "        ・1: 科学技術\n",
    "        ・2: エンターテイメント\n",
    "        ・3: 健康\n",
    "このX, Yをtrain, valid, testに分割されたデータに対して作る\n",
    "データの形式\n",
    "    ・colum1: タイトル\n",
    "    ・colum2: カテゴリ\n",
    "        ・b: ビジネス\n",
    "        ・t: 科学技術\n",
    "        ・e: エンターテイメント\n",
    "        ・m: 健康\n",
    "'''\n",
    "\n",
    "# データの読み込み\n",
    "def generate_case(file_name):\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            yield line.rstrip().split('\\t')\n",
    "\n",
    "# 単語の特徴ベクトルへの変換\n",
    "def word_to_feature(word):\n",
    "    try:\n",
    "        return word2vec_model[word]\n",
    "    except:\n",
    "        # print('not found the word: {}'.format(word))\n",
    "        return np.zeros(300)\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = re.sub(r'[\\\"\\'.,:;\\(\\)#\\|\\*\\+\\!\\?#$%&/\\]\\[\\{\\}]', '', text)\n",
    "    text = re.sub(r'\\s-\\s', ' ', text)\n",
    "    text = re.sub(r'\\t', '', text)\n",
    "    return text\n",
    "\n",
    "# タイトルの特徴ベクトルへの変換→word_to_featureを平均したもの\n",
    "def title_to_features(title):\n",
    "    features = np.array([word_to_feature(word) for word in preprocessing(title).split(' ')]).mean(axis=0)\n",
    "    return torch.from_numpy(features.astype(np.float32)).clone()\n",
    "\n",
    "# カテゴリをラベルに変換\n",
    "def category_to_label(category):\n",
    "    labels = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "    return labels[category]\n",
    "\n",
    "def save_data(X, Y, file_name):\n",
    "    # torchで保存\n",
    "    torch.save(X, file_name + '_X.pt')\n",
    "    torch.save(Y, file_name + '_Y.pt')\n",
    "\n",
    "def main(file_path):\n",
    "    global word2vec_model\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format('/work01/s-nozue/100knock/chapter07/data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    X = []\n",
    "    Y = []\n",
    "    output_file_path = file_path.replace('.txt', '')\n",
    "    count = 0\n",
    "\n",
    "    for title, category in generate_case(file_path):\n",
    "        X.append(title_to_features(title))\n",
    "        Y.append(category_to_label(category))\n",
    "        count += 1\n",
    "\n",
    "    X = torch.stack(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    save_data(X, Y, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('/work01/s-nozue/100knock/chapter08/datasets/valid_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('/work01/s-nozue/100knock/chapter08/datasets/test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('/work01/s-nozue/100knock/chapter08/datasets/train_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 71. 単層ニューラルネットワークによる予測\n",
    "問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def cal_softmax(x):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    W = torch.randn(300, 4, requires_grad=True)\n",
    "    xw = torch.matmul(x, W)\n",
    "    return softmax(xw)\n",
    "\n",
    "def main(file_path):\n",
    "    X_train = torch.load(file_path)\n",
    "    X_train1 = X_train[:1]\n",
    "    X_train4 = X_train[:4]\n",
    "    Y_hat1 = cal_softmax(X_train1)\n",
    "    Y_hat4 = cal_softmax(X_train4)\n",
    "    \n",
    "    print('Y_hat1', Y_hat1, 'sum=', Y_hat1.sum(dim=1))\n",
    "    print('Y_hat4', Y_hat4, 'sum=', Y_hat4.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_hat1 tensor([[0.1914, 0.5460, 0.1230, 0.1396]], grad_fn=<SoftmaxBackward0>) sum= tensor([1.0000], grad_fn=<SumBackward1>)\n",
      "Y_hat4 tensor([[0.0362, 0.1027, 0.8457, 0.0154],\n",
      "        [0.3690, 0.4155, 0.1561, 0.0594],\n",
      "        [0.0508, 0.0187, 0.8943, 0.0362],\n",
      "        [0.3417, 0.1126, 0.3589, 0.1869]], grad_fn=<SoftmaxBackward0>) sum= tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "main('/work01/s-nozue/100knock/chapter08/datasets/train_data_X.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 72. 損失と勾配の計算\n",
    "学習データの事例x1と事例集合x1,x2,x3,x4に対して，クロスエントロピー損失と，行列Wに対する勾配を計算せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def cal_cross_entropy_loss(y_hat, y):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return criterion(y_hat, y)\n",
    "\n",
    "def cal_gradient(y_hat, y, x):\n",
    "    loss = cal_cross_entropy_loss(y_hat, y)\n",
    "    print('loss:', loss)\n",
    "    loss.backward()\n",
    "    return x.grad\n",
    "\n",
    "def main(X_file_path, Y_file_path):\n",
    "    X_train = torch.load(X_file_path)\n",
    "    Y_train = torch.load(Y_file_path)\n",
    "    X_train1 = X_train[:1].requires_grad_()\n",
    "    X_train4 = X_train[:4].requires_grad_()\n",
    "    Y1 = Y_train[:1]\n",
    "    Y4 = Y_train[:4]\n",
    "    Y_hat1 = cal_softmax(X_train1)\n",
    "    Y_hat4 = cal_softmax(X_train4)\n",
    "    \n",
    "    print('========== Y_hat1 ==========')\n",
    "    print(Y_hat1, '->', torch.argmax(Y_hat1, dim=1), '\"gold\":', Y1)\n",
    "    print('grad:', cal_gradient(Y_hat1, Y1, X_train1))\n",
    "\n",
    "    print('========== Y_hat4 ==========')\n",
    "    print( Y_hat4, '->', torch.argmax(Y_hat4, dim=1), '\"gold\":', Y4)\n",
    "    print('grad:', cal_gradient(Y_hat4, Y4, X_train4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Y_hat1 ==========\n",
      "tensor([[0.5703, 0.1023, 0.1865, 0.1409]], grad_fn=<SoftmaxBackward0>) -> tensor([0]) \"gold\": tensor([2])\n",
      "loss: tensor(1.4685, grad_fn=<NllLossBackward0>)\n",
      "grad: tensor([[-0.1775,  0.0140,  0.0729,  0.2089, -0.7135,  0.2121, -0.2800,  0.0240,\n",
      "          0.1265,  0.3297,  0.1750,  0.1489, -0.2258,  0.2805,  0.2140, -0.2243,\n",
      "         -0.0369, -0.0042, -0.0397, -0.0677, -0.3192, -0.0222, -0.2185, -0.2882,\n",
      "          0.1209,  0.0011, -0.3448,  0.2535, -0.0590, -0.2058, -0.0021,  0.2254,\n",
      "          0.0986, -0.2308, -0.1962, -0.0063, -0.0725,  0.3029,  0.1621, -0.1827,\n",
      "         -0.0292, -0.0443, -0.2321, -0.1987, -0.3241,  0.3008,  0.0024, -0.3606,\n",
      "          0.3592, -0.1538,  0.0013,  0.0766, -0.0933,  0.3255, -0.2350, -0.1914,\n",
      "          0.1532, -0.0225, -0.0930,  0.0206, -0.0398,  0.1111, -0.2608,  0.3561,\n",
      "          0.0799, -0.3846,  0.1436, -0.2042, -0.0719, -0.4149,  0.0688,  0.0871,\n",
      "          0.0519,  0.0973, -0.0990,  0.0931, -0.0942,  0.1578, -0.2250,  0.1430,\n",
      "          0.3890, -0.0470, -0.1618, -0.1753,  0.0646,  0.2741,  0.2934,  0.0478,\n",
      "         -0.1782,  0.1697, -0.1944, -0.0863, -0.0707, -0.3965, -0.4846,  0.3428,\n",
      "          0.1379,  0.2381,  0.1329, -0.2078,  0.2688,  0.2184, -0.0604, -0.1923,\n",
      "         -0.0254, -0.0243, -0.2590, -0.2808, -0.1664, -0.0975,  0.3856,  0.1260,\n",
      "          0.0699,  0.0320, -0.2116, -0.1263,  0.1282, -0.3731,  0.0285, -0.1761,\n",
      "         -0.0685,  0.0162, -0.1842, -0.1887, -0.2398, -0.0320, -0.0910,  0.0878,\n",
      "         -0.2392,  0.1500,  0.2695,  0.2225, -0.4164,  0.1157,  0.0412,  0.2097,\n",
      "         -0.0174,  0.1509,  0.1726,  0.0743, -0.1159,  0.0988,  0.0842, -0.1901,\n",
      "          0.0469, -0.2580,  0.0177,  0.2061,  0.3511,  0.1144,  0.0950,  0.6296,\n",
      "          0.0901,  0.1865,  0.1885, -0.1601,  0.1266,  0.1903, -0.0639, -0.2070,\n",
      "          0.0303,  0.2208,  0.2256,  0.2617, -0.2365, -0.0284, -0.2357,  0.3471,\n",
      "          0.2067, -0.0652,  0.2113,  0.2271,  0.1013, -0.1685,  0.1408, -0.0710,\n",
      "         -0.0257,  0.2138, -0.2763, -0.0261,  0.0031,  0.0022,  0.2550,  0.0565,\n",
      "          0.3204, -0.1578, -0.1484, -0.0044,  0.2780,  0.2210, -0.1296, -0.2924,\n",
      "          0.2258, -0.0250, -0.0184, -0.0386,  0.1553, -0.2510,  0.0683, -0.3866,\n",
      "          0.1559, -0.1269,  0.0533, -0.1900,  0.1962,  0.0123,  0.0563,  0.0595,\n",
      "          0.1091, -0.0396,  0.3692,  0.2776, -0.0867,  0.2180,  0.0423, -0.2044,\n",
      "          0.1860,  0.2342,  0.0820,  0.1612, -0.0635, -0.0921,  0.1830,  0.1812,\n",
      "         -0.3510, -0.2554, -0.2773,  0.0170,  0.3248, -0.0833,  0.0656,  0.5185,\n",
      "         -0.0416, -0.1075, -0.0108,  0.2554, -0.1853, -0.1332,  0.0609, -0.0036,\n",
      "         -0.1206,  0.0740, -0.0690, -0.1586, -0.1971, -0.0553,  0.0095,  0.3506,\n",
      "         -0.5316,  0.1176,  0.4323,  0.0008, -0.4049,  0.1835, -0.1628,  0.0754,\n",
      "         -0.3754,  0.3966,  0.4033, -0.0526, -0.0359,  0.1017, -0.4690, -0.2535,\n",
      "          0.0312, -0.3416,  0.1760, -0.1221, -0.1144,  0.0312, -0.1113, -0.1823,\n",
      "         -0.0051,  0.0135,  0.3146, -0.0449,  0.4503, -0.1708, -0.1265,  0.0919,\n",
      "          0.2330, -0.2002, -0.0707,  0.1030, -0.0411,  0.2135, -0.0624,  0.3107,\n",
      "         -0.0526, -0.4773, -0.5105, -0.2554,  0.2787,  0.4088, -0.0776, -0.2742,\n",
      "          0.1660,  0.1571, -0.2649,  0.0908]])\n",
      "========== Y_hat4 ==========\n",
      "tensor([[0.2727, 0.4941, 0.0610, 0.1722],\n",
      "        [0.1041, 0.6818, 0.0863, 0.1278],\n",
      "        [0.4324, 0.0728, 0.2691, 0.2257],\n",
      "        [0.1382, 0.1498, 0.3010, 0.4110]], grad_fn=<SoftmaxBackward0>) -> tensor([1, 1, 0, 3]) \"gold\": tensor([2, 3, 2, 1])\n",
      "loss: tensor(1.4998, grad_fn=<NllLossBackward0>)\n",
      "grad: tensor([[ 0.0214,  0.0213, -0.0232,  ..., -0.0492, -0.0019, -0.0077],\n",
      "        [ 0.0248,  0.0438, -0.0180,  ..., -0.0799, -0.0023, -0.0221],\n",
      "        [ 0.0334,  0.0637, -0.0946,  ..., -0.1044, -0.0018,  0.0349],\n",
      "        [-0.0370, -0.0392,  0.0216,  ...,  0.0899,  0.0036,  0.0312]])\n"
     ]
    }
   ],
   "source": [
    "main('/work01/s-nozue/100knock/chapter08/datasets/train_data_X.pt', '/work01/s-nozue/100knock/chapter08/datasets/train_data_Y.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列Wを学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "import timm.scheduler\n",
    "# import wandb\n",
    "\n",
    "class NetWork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetWork, self).__init__()\n",
    "        self.fc = nn.Linear(300, 4)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "def main(X_file_path, Y_file_path):\n",
    "    # wandb.init(project='100knock-2023')\n",
    "    X_train = torch.load(X_file_path)\n",
    "    Y_train = torch.load(Y_file_path)\n",
    "    net = NetWork()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "    scheduler = timm.scheduler.StepLRScheduler(optimizer, warmup_t=5, warmup_lr_init=0, decay_t=1, decay_rate = 0.5,)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        for X, y in zip(X_train, Y_train):\n",
    "            optimizer.zero_grad()\n",
    "            output = net(X)\n",
    "            loss = cal_cross_entropy_loss(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step(epoch + 1)\n",
    "        print(f'epoch: {epoch + 1}, loss: {loss.item()}')\n",
    "        # wandb.log({'loss': loss.item(), 'lr': scheduler.get_lr()[0]})\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.3674858808517456\n",
      "epoch: 2, loss: 1.0281847715377808\n",
      "epoch: 3, loss: 0.73114013671875\n",
      "epoch: 4, loss: 0.5134780406951904\n",
      "epoch: 5, loss: 0.35844722390174866\n",
      "epoch: 6, loss: 0.24946166574954987\n",
      "epoch: 7, loss: 0.21100321412086487\n",
      "epoch: 8, loss: 0.1950041800737381\n",
      "epoch: 9, loss: 0.1877036988735199\n",
      "epoch: 10, loss: 0.18421433866024017\n"
     ]
    }
   ],
   "source": [
    "net = main('/work01/s-nozue/100knock/chapter08/datasets/train_data_X.pt', '/work01/s-nozue/100knock/chapter08/datasets/train_data_Y.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 74. 正解率の計測\n",
    "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, X_eval, Y_eval):\n",
    "    with torch.no_grad():\n",
    "        output = model(X_eval)\n",
    "        loss = cal_cross_entropy_loss(output, Y_eval)\n",
    "        accuracy = (torch.argmax(output, dim=1) == Y_eval).sum().item() / len(Y_eval)\n",
    "        return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.781437125748503\n"
     ]
    }
   ],
   "source": [
    "X_eval = torch.load('/work01/s-nozue/100knock/chapter08/datasets/valid_data_X.pt')\n",
    "Y_eval = torch.load('/work01/s-nozue/100knock/chapter08/datasets/valid_data_Y.pt')\n",
    "loss, accuracy = eval(net, X_eval, Y_eval)\n",
    "print(f'accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 75. 損失と正解率のプロット\n",
    "問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "class NetWork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetWork, self).__init__()\n",
    "        self.fc = nn.Linear(300, 4)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "def main(X_file_path, Y_file_path):\n",
    "    wandb.init(project='100knock-2023')\n",
    "    X_train = torch.load(X_file_path)\n",
    "    Y_train = torch.load(Y_file_path)\n",
    "    X_eval = torch.load('/work01/s-nozue/100knock/chapter08/datasets/valid_data_X.pt')\n",
    "    Y_eval = torch.load('/work01/s-nozue/100knock/chapter08/datasets/valid_data_Y.pt')\n",
    "    net = NetWork()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "    scheduler = timm.scheduler.CosineLRScheduler(optimizer, warmup_t=5, warmup_lr_init=0, t_initial=20)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        for X, y in zip(X_train, Y_train):\n",
    "            optimizer.zero_grad()\n",
    "            output = net(X)\n",
    "            loss = cal_cross_entropy_loss(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        eval_loss, acc = eval(net, X_eval, Y_eval)\n",
    "        wandb.log({'train_loss': loss.item(), 'eval_loss': eval_loss.item(), 'accuracy': acc, 'lr:': optimizer.param_groups[0][\"lr\"]})\n",
    "        print(f'epoch: {epoch + 1}, train_loss: {loss.item()}, eval_loss: {eval_loss}, accuracy: {acc}')\n",
    "        scheduler.step(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ybvswxtg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▇██████████████</td></tr><tr><td>eval_loss</td><td>█▆▅▄▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>lr:</td><td>▁▃▄▆███▇▆▆▅▄▄▃▃▂</td></tr><tr><td>train_loss</td><td>█▆▄▃▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.78593</td></tr><tr><td>eval_loss</td><td>0.61378</td></tr><tr><td>lr:</td><td>0.00015</td></tr><tr><td>train_loss</td><td>0.08046</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-mountain-24</strong> at: <a href='https://wandb.ai/edunlp-tohoku/100knock-2023/runs/ybvswxtg' target=\"_blank\">https://wandb.ai/edunlp-tohoku/100knock-2023/runs/ybvswxtg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231018_154521-ybvswxtg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ybvswxtg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work01/s-nozue/100knock/chapter08/wandb/run-20231018_154636-6su8yzdy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/edunlp-tohoku/100knock-2023/runs/6su8yzdy' target=\"_blank\">likely-durian-25</a></strong> to <a href='https://wandb.ai/edunlp-tohoku/100knock-2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/edunlp-tohoku/100knock-2023' target=\"_blank\">https://wandb.ai/edunlp-tohoku/100knock-2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/edunlp-tohoku/100knock-2023/runs/6su8yzdy' target=\"_blank\">https://wandb.ai/edunlp-tohoku/100knock-2023/runs/6su8yzdy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 1.3298213481903076, eval_loss: 1.3904500007629395, accuracy: 0.14520958083832336\n",
      "epoch: 2, train_loss: 0.3355235457420349, eval_loss: 0.8342428803443909, accuracy: 0.782185628742515\n",
      "epoch: 3, train_loss: 0.10837189853191376, eval_loss: 0.6497262716293335, accuracy: 0.7874251497005988\n",
      "epoch: 4, train_loss: 0.04066023975610733, eval_loss: 0.5440211892127991, accuracy: 0.8106287425149701\n",
      "epoch: 5, train_loss: 0.017862994223833084, eval_loss: 0.4744621217250824, accuracy: 0.843562874251497\n",
      "epoch: 6, train_loss: 0.009713172912597656, eval_loss: 0.4322569966316223, accuracy: 0.8645209580838323\n",
      "epoch: 7, train_loss: 0.006270141340792179, eval_loss: 0.40655645728111267, accuracy: 0.8697604790419161\n",
      "epoch: 8, train_loss: 0.004493259359151125, eval_loss: 0.3894203007221222, accuracy: 0.8794910179640718\n",
      "epoch: 9, train_loss: 0.003457641461864114, eval_loss: 0.37732452154159546, accuracy: 0.8824850299401198\n",
      "epoch: 10, train_loss: 0.0028038020245730877, eval_loss: 0.36848321557044983, accuracy: 0.8847305389221557\n",
      "epoch: 11, train_loss: 0.002367552602663636, eval_loss: 0.3618934452533722, accuracy: 0.8847305389221557\n",
      "epoch: 12, train_loss: 0.002065312582999468, eval_loss: 0.35694602131843567, accuracy: 0.8854790419161677\n",
      "epoch: 13, train_loss: 0.0018509175861254334, eval_loss: 0.35324615240097046, accuracy: 0.8869760479041916\n",
      "epoch: 14, train_loss: 0.0016977671766653657, eval_loss: 0.3505215346813202, accuracy: 0.8862275449101796\n",
      "epoch: 15, train_loss: 0.00158946483861655, eval_loss: 0.3485718369483948, accuracy: 0.8862275449101796\n",
      "epoch: 16, train_loss: 0.001515550771728158, eval_loss: 0.3472398817539215, accuracy: 0.8854790419161677\n",
      "epoch: 17, train_loss: 0.0014686522772535682, eval_loss: 0.34639471769332886, accuracy: 0.8854790419161677\n",
      "epoch: 18, train_loss: 0.0014419882791116834, eval_loss: 0.34592050313949585, accuracy: 0.8854790419161677\n",
      "epoch: 19, train_loss: 0.0014299653703346848, eval_loss: 0.34570956230163574, accuracy: 0.8854790419161677\n",
      "epoch: 20, train_loss: 0.001426870352588594, eval_loss: 0.34565702080726624, accuracy: 0.8854790419161677\n"
     ]
    }
   ],
   "source": [
    "main('/work01/s-nozue/100knock/chapter08/datasets/train_data_X.pt', '/work01/s-nozue/100knock/chapter08/datasets/train_data_Y.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 76. チェックポイント\n",
    "問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def save_checkpoint(model, optimizer, scheduler, file_name):\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, file_name)\n",
    "\n",
    "def main(X_file_path, Y_file_path):\n",
    "    # wandb.init(project='100knock-2023')\n",
    "    X_train = torch.load(X_file_path)\n",
    "    Y_train = torch.load(Y_file_path)\n",
    "    X_eval = torch.load('/work01/s-nozue/100knock/chapter08/datasets/valid_data_X.pt')\n",
    "    Y_eval = torch.load('/work01/s-nozue/100knock/chapter08/datasets/valid_data_Y.pt')\n",
    "    net = NetWork()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "    scheduler = timm.scheduler.CosineLRScheduler(optimizer, warmup_t=5, warmup_lr_init=0, t_initial=20)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        for X, y in zip(X_train, Y_train):\n",
    "            optimizer.zero_grad()\n",
    "            output = net(X)\n",
    "            loss = cal_cross_entropy_loss(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        eval_loss, acc = eval(net, X_eval, Y_eval)\n",
    "        # wandb.log({'train_loss': loss.item(), 'eval_loss': eval_loss.item(), 'accuracy': acc, 'lr:': optimizer.param_groups[0][\"lr\"]})\n",
    "        print(f'epoch: {epoch + 1}, train_loss: {loss.item()}, eval_loss: {eval_loss}, accuracy: {acc}')\n",
    "        scheduler.step(epoch + 1)\n",
    "        save_checkpoint(net, optimizer, scheduler, f'/work01/s-nozue/100knock/chapter08/models/checkpoint_{epoch + 1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 1.4502325057983398, eval_loss: 1.3994134664535522, accuracy: 0.14221556886227546\n",
      "epoch: 2, train_loss: 0.371903657913208, eval_loss: 0.8329234719276428, accuracy: 0.7799401197604791\n",
      "epoch: 3, train_loss: 0.1205444484949112, eval_loss: 0.6488741040229797, accuracy: 0.7844311377245509\n",
      "epoch: 4, train_loss: 0.04491501674056053, eval_loss: 0.5432769656181335, accuracy: 0.8106287425149701\n",
      "epoch: 5, train_loss: 0.019581111147999763, eval_loss: 0.4738258719444275, accuracy: 0.8473053892215568\n",
      "epoch: 6, train_loss: 0.010583464987576008, eval_loss: 0.4317111670970917, accuracy: 0.8645209580838323\n",
      "epoch: 7, train_loss: 0.006802614312618971, eval_loss: 0.4060709476470947, accuracy: 0.8705089820359282\n",
      "epoch: 8, train_loss: 0.00485918577760458, eval_loss: 0.38897302746772766, accuracy: 0.8772455089820359\n",
      "epoch: 9, train_loss: 0.003730245167389512, eval_loss: 0.3769018054008484, accuracy: 0.8817365269461078\n",
      "epoch: 10, train_loss: 0.0030189435929059982, eval_loss: 0.3680766522884369, accuracy: 0.8839820359281437\n",
      "epoch: 11, train_loss: 0.002545333234593272, eval_loss: 0.3614973723888397, accuracy: 0.8862275449101796\n",
      "epoch: 12, train_loss: 0.0022176930215209723, eval_loss: 0.3565566837787628, accuracy: 0.8862275449101796\n",
      "epoch: 13, train_loss: 0.00198560394346714, eval_loss: 0.35286110639572144, accuracy: 0.8869760479041916\n",
      "epoch: 14, train_loss: 0.0018198610050603747, eval_loss: 0.35013917088508606, accuracy: 0.8877245508982036\n",
      "epoch: 15, train_loss: 0.0017028844449669123, eval_loss: 0.34819111227989197, accuracy: 0.8869760479041916\n",
      "epoch: 16, train_loss: 0.0016231469344347715, eval_loss: 0.34686043858528137, accuracy: 0.8854790419161677\n",
      "epoch: 17, train_loss: 0.001572206849232316, eval_loss: 0.3460162580013275, accuracy: 0.8847305389221557\n",
      "epoch: 18, train_loss: 0.0015434031374752522, eval_loss: 0.3455429673194885, accuracy: 0.8847305389221557\n",
      "epoch: 19, train_loss: 0.0015304292319342494, eval_loss: 0.3453325629234314, accuracy: 0.8847305389221557\n",
      "epoch: 20, train_loss: 0.001527215470559895, eval_loss: 0.34528014063835144, accuracy: 0.8847305389221557\n"
     ]
    }
   ],
   "source": [
    "main('/work01/s-nozue/100knock/chapter08/datasets/train_data_X.pt', '/work01/s-nozue/100knock/chapter08/datasets/train_data_Y.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 77. ミニバッチ化\n",
    "問題76のコードを改変し，B事例ごとに損失・勾配を計算し，行列Wの値を更新せよ（ミニバッチ化）．Bの値を1,2,4,8,…と変化させながら，1エポックの学習に要する時間を比較せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work01/s-nozue/100knock/chapter08/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "import timm.scheduler\n",
    "import time\n",
    "\n",
    "class NetWork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetWork, self).__init__()\n",
    "        self.fc = nn.Linear(300, 4)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "def save_checkpoint(model, optimizer, scheduler, file_name):\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, file_name)\n",
    "\n",
    "def data_loader(x, y, batch_size):\n",
    "    x_loader = torch.utils.data.DataLoader(x, batch_size=batch_size)\n",
    "    y_loader = torch.utils.data.DataLoader(y, batch_size=batch_size)\n",
    "    return x_loader, y_loader\n",
    "\n",
    "def cal_cross_entropy_loss(y_hat, y):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return criterion(y_hat, y)\n",
    "\n",
    "def cal_gradient(y_hat, y, x):\n",
    "    loss = cal_cross_entropy_loss(y_hat, y)\n",
    "    print('loss:', loss)\n",
    "    loss.backward()\n",
    "    return x.grad\n",
    "\n",
    "def eval(model, X_eval, Y_eval):\n",
    "    with torch.no_grad():\n",
    "        output = model(X_eval)\n",
    "        loss = cal_cross_entropy_loss(output, Y_eval)\n",
    "        accuracy = (torch.argmax(output, dim=1) == Y_eval).sum().item() / len(Y_eval)\n",
    "        return loss, accuracy\n",
    "\n",
    "def main(X_file_path, Y_file_path, batch_size=1):\n",
    "    # wandb.init(project='100knock-2023')\n",
    "    X_train = torch.load(X_file_path)\n",
    "    Y_train = torch.load(Y_file_path)\n",
    "    x_loader_train, y_loader_train = data_loader(X_train, Y_train, batch_size)\n",
    "\n",
    "    X_eval = torch.load('/work01/s-nozue/100knock/chapter08/datasets/valid_data_X.pt')\n",
    "    Y_eval = torch.load('/work01/s-nozue/100knock/chapter08/datasets/valid_data_Y.pt')\n",
    "    # x_loader_eval, y_loader_eval = data_loader(X_eval, Y_eval, batch_size)\n",
    "    \n",
    "    net = NetWork()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "    scheduler = timm.scheduler.CosineLRScheduler(optimizer, warmup_t=5, warmup_lr_init=0, t_initial=20)\n",
    "\n",
    "    for epoch in range(1):\n",
    "        start_time = time.time()\n",
    "        for X, y in zip(x_loader_train, y_loader_train):\n",
    "            optimizer.zero_grad()\n",
    "            output = net(X)\n",
    "            loss = cal_cross_entropy_loss(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end_time = time.time()\n",
    "        print(f'epoch: {epoch + 1}, time: {end_time - start_time}')\n",
    "        eval_loss, acc = eval(net, X_eval, Y_eval)\n",
    "        # wandb.log({'train_loss': loss.item(), 'eval_loss': eval_loss.item(), 'accuracy': acc, 'lr:': optimizer.param_groups[0][\"lr\"]})\n",
    "        print(f'epoch: {epoch + 1}, train_loss: {loss.item()}, eval_loss: {eval_loss}, accuracy: {acc}')\n",
    "        scheduler.step(epoch + 1)\n",
    "        save_checkpoint(net, optimizer, scheduler, f'/work01/s-nozue/100knock/chapter08/models/checkpoint_77_{epoch + 1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, time: 4.593250513076782\n",
      "epoch: 1, train_loss: 1.3006435632705688, eval_loss: 1.3776284456253052, accuracy: 0.2657185628742515\n"
     ]
    }
   ],
   "source": [
    "main('/work01/s-nozue/100knock/chapter08/datasets/train_data_X.pt', '/work01/s-nozue/100knock/chapter08/datasets/train_data_Y.pt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, time: 2.686436414718628\n",
      "epoch: 1, train_loss: 1.350126028060913, eval_loss: 1.3743897676467896, accuracy: 0.38772455089820357\n"
     ]
    }
   ],
   "source": [
    "main('/work01/s-nozue/100knock/chapter08/datasets/train_data_X.pt', '/work01/s-nozue/100knock/chapter08/datasets/train_data_Y.pt', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, time: 1.5043866634368896\n",
      "epoch: 1, train_loss: 1.395340919494629, eval_loss: 1.3925906419754028, accuracy: 0.2597305389221557\n"
     ]
    }
   ],
   "source": [
    "main('/work01/s-nozue/100knock/chapter08/datasets/train_data_X.pt', '/work01/s-nozue/100knock/chapter08/datasets/train_data_Y.pt', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, time: 0.6768782138824463\n",
      "epoch: 1, train_loss: 1.3528156280517578, eval_loss: 1.3854172229766846, accuracy: 0.19011976047904192\n"
     ]
    }
   ],
   "source": [
    "main('/work01/s-nozue/100knock/chapter08/datasets/train_data_X.pt', '/work01/s-nozue/100knock/chapter08/datasets/train_data_Y.pt', 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 78. GPU上での学習\n",
    "問題77のコードを改変し，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(X_file_path, Y_file_path, batch_size=1):\n",
    "    # wandb.init(project='100knock-2023')\n",
    "    X_train = torch.load(X_file_path)\n",
    "    Y_train = torch.load(Y_file_path)\n",
    "    x_loader_train, y_loader_train = data_loader(X_train, Y_train, batch_size)\n",
    "\n",
    "    X_eval = torch.load('/work01/s-nozue/100knock/chapter08/datasets/valid_data_X.pt')\n",
    "    Y_eval = torch.load('/work01/s-nozue/100knock/chapter08/datasets/valid_data_Y.pt')\n",
    "    # x_loader_eval, y_loader_eval = data_loader(X_eval, Y_eval, batch_size)\n",
    "    \n",
    "    net = NetWork()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "    scheduler = timm.scheduler.CosineLRScheduler(optimizer, warmup_t=5, warmup_lr_init=0, t_initial=20)\n",
    "\n",
    "    # netをGPUに載せる \n",
    "    net.to('cuda')\n",
    "    X_eval = X_eval.to('cuda')\n",
    "    Y_eval = Y_eval.to('cuda')\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        start_time = time.time()\n",
    "        for X, y in zip(x_loader_train, y_loader_train):\n",
    "            X = X.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            output = net(X)\n",
    "            loss = cal_cross_entropy_loss(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end_time = time.time()\n",
    "        print(f'epoch: {epoch + 1}, time: {end_time - start_time}')\n",
    "        eval_loss, acc = eval(net, X_eval, Y_eval)\n",
    "        # wandb.log({'train_loss': loss.item(), 'eval_loss': eval_loss.item(), 'accuracy': acc, 'lr:': optimizer.param_groups[0][\"lr\"]})\n",
    "        print(f'epoch: {epoch + 1}, train_loss: {loss.item()}, eval_loss: {eval_loss}, accuracy: {acc}')\n",
    "        scheduler.step(epoch + 1)\n",
    "        save_checkpoint(net, optimizer, scheduler, f'/work01/s-nozue/100knock/chapter08/models/checkpoint_77_{epoch + 1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, time: 1.7512094974517822\n",
      "epoch: 1, train_loss: 1.3964546918869019, eval_loss: 1.3819477558135986, accuracy: 0.23802395209580837\n",
      "epoch: 2, time: 1.7681827545166016\n",
      "epoch: 2, train_loss: 1.182250738143921, eval_loss: 1.1406705379486084, accuracy: 0.7552395209580839\n",
      "epoch: 3, time: 1.7972397804260254\n",
      "epoch: 3, train_loss: 1.0387433767318726, eval_loss: 0.9954099655151367, accuracy: 0.7761976047904192\n",
      "epoch: 4, time: 1.7793617248535156\n",
      "epoch: 4, train_loss: 0.9156491756439209, eval_loss: 0.8854034543037415, accuracy: 0.7776946107784432\n",
      "epoch: 5, time: 1.7892577648162842\n",
      "epoch: 5, train_loss: 0.809401273727417, eval_loss: 0.7968606352806091, accuracy: 0.780688622754491\n",
      "epoch: 6, time: 1.7966270446777344\n",
      "epoch: 6, train_loss: 0.7329024076461792, eval_loss: 0.7354108691215515, accuracy: 0.781437125748503\n",
      "epoch: 7, time: 1.7939023971557617\n",
      "epoch: 7, train_loss: 0.6811598539352417, eval_loss: 0.6940565705299377, accuracy: 0.7829341317365269\n",
      "epoch: 8, time: 1.7914259433746338\n",
      "epoch: 8, train_loss: 0.6442844271659851, eval_loss: 0.6641799807548523, accuracy: 0.7829341317365269\n",
      "epoch: 9, time: 1.7382915019989014\n",
      "epoch: 9, train_loss: 0.6170214414596558, eval_loss: 0.6417131423950195, accuracy: 0.7851796407185628\n",
      "epoch: 10, time: 1.7652969360351562\n",
      "epoch: 10, train_loss: 0.5964066982269287, eval_loss: 0.6244542002677917, accuracy: 0.7851796407185628\n"
     ]
    }
   ],
   "source": [
    "main('/work01/s-nozue/100knock/chapter08/datasets/train_data_X.pt', '/work01/s-nozue/100knock/chapter08/datasets/train_data_Y.pt', 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 79. 多層ニューラルネットワーク\n",
    "問題78のコードを改変し，バイアス項の導入や多層化など，ニューラルネットワークの形状を変更しながら，高性能なカテゴリ分類器を構築せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class NetWork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetWork, self).__init__()\n",
    "        self.fc0 = nn.Linear(300, 150)\n",
    "        self.fc1 = nn.Linear(150, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 4)\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc0(x))\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def main(X_file_path, Y_file_path, batch_size=1):\n",
    "    # wandb.init(project='100knock-2023')\n",
    "    X_train = torch.load(X_file_path)\n",
    "    Y_train = torch.load(Y_file_path)\n",
    "    x_loader_train, y_loader_train = data_loader(X_train, Y_train, batch_size)\n",
    "\n",
    "    X_eval = torch.load('/work01/s-nozue/100knock/chapter08/datasets/valid_data_X.pt')\n",
    "    Y_eval = torch.load('/work01/s-nozue/100knock/chapter08/datasets/valid_data_Y.pt')\n",
    "    # x_loader_eval, y_loader_eval = data_loader(X_eval, Y_eval, batch_size)\n",
    "    \n",
    "    net = NetWork()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "    scheduler = timm.scheduler.CosineLRScheduler(optimizer, warmup_t=5, warmup_lr_init=0, t_initial=20)\n",
    "\n",
    "    # netをGPUに載せる \n",
    "    net.to('cuda')\n",
    "    X_eval = X_eval.to('cuda')\n",
    "    Y_eval = Y_eval.to('cuda')\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        start_time = time.time()\n",
    "        for X, y in zip(x_loader_train, y_loader_train):\n",
    "            X = X.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            output = net(X)\n",
    "            loss = cal_cross_entropy_loss(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end_time = time.time()\n",
    "        print(f'epoch: {epoch + 1}, time: {end_time - start_time}')\n",
    "        eval_loss, acc = eval(net, X_eval, Y_eval)\n",
    "        # wandb.log({'train_loss': loss.item(), 'eval_loss': eval_loss.item(), 'accuracy': acc, 'lr:': optimizer.param_groups[0][\"lr\"]})\n",
    "        print(f'epoch: {epoch + 1}, train_loss: {loss.item()}, eval_loss: {eval_loss}, accuracy: {acc}')\n",
    "        scheduler.step(epoch + 1)\n",
    "        save_checkpoint(net, optimizer, scheduler, f'/work01/s-nozue/100knock/chapter08/models/checkpoint_77_{epoch + 1}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zar_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
